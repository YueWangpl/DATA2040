{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.041374,
     "end_time": "2021-03-21T22:14:56.763089",
     "exception": false,
     "start_time": "2021-03-21T22:14:56.721715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Base code: https://www.kaggle.com/jessemostipak/getting-started-tpus-cassava-leaf-disease "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "papermill": {
     "duration": 0.037962,
     "end_time": "2021-03-21T22:14:56.839875",
     "exception": false,
     "start_time": "2021-03-21T22:14:56.801913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tensor Processing Units (TPUs)\n",
    "\n",
    "Tensor Processing Units (TPUs) are hardware accelerators that are specialized for deep learning tasks. All Kagglers have 30 hours of free TPU time each week, and can use up to 3 hours in a single session (although if you'd like to increase your TPU quota consider submitting an exemplary TPU notebook to our **[TPU Star program](https://www.kaggle.com/tpu-prize)**!)   \n",
    "\n",
    "You can read through the Kaggle documentation on TPUs **[here](https://www.kaggle.com/docs/tpu)**, and check out the TPU Star notebooks **[here](https://www.kaggle.com/tpu-stars)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036565,
     "end_time": "2021-03-21T22:14:56.913826",
     "exception": false,
     "start_time": "2021-03-21T22:14:56.877261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:14:56.992324Z",
     "iopub.status.busy": "2021-03-21T22:14:56.991558Z",
     "iopub.status.idle": "2021-03-21T22:15:04.209985Z",
     "shell.execute_reply": "2021-03-21T22:15:04.211474Z"
    },
    "papermill": {
     "duration": 7.263236,
     "end_time": "2021-03-21T22:15:04.211814",
     "exception": false,
     "start_time": "2021-03-21T22:14:56.948578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import math, re, os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038503,
     "end_time": "2021-03-21T22:15:04.290255",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.251752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Detect TPU\n",
    "What we're doing with our code here is making sure that we'll be sending our data across a TPU. What you're looking for is a printout of `Number of replicas: 8`, corresponding to the 8 cores of a TPU. If your printout instead says `Number of replicas: 1` you likely do not have TPUs enabled in your notebook.   \n",
    "\n",
    "To enable TPUs navigate to the panel on the right and click on `Accelerator`. Choose TPU from the dropdown.  \n",
    "\n",
    "If you'd like more TPU troubleshooting and optimization guidelines check out our **[Learn With Me: Troubleshooting and Optimizing TPUs video](https://youtu.be/BSeWHzjMHMU)**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:04.372131Z",
     "iopub.status.busy": "2021-03-21T22:15:04.371466Z",
     "iopub.status.idle": "2021-03-21T22:15:04.377012Z",
     "shell.execute_reply": "2021-03-21T22:15:04.376186Z"
    },
    "papermill": {
     "duration": 0.048817,
     "end_time": "2021-03-21T22:15:04.377177",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.328360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#     print('Device:', tpu.master())\n",
    "#     tf.config.experimental_connect_to_cluster(tpu)\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "# except:\n",
    "#     strategy = tf.distribute.get_strategy()\n",
    "# print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040677,
     "end_time": "2021-03-21T22:15:04.457429",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.416752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set up variables\n",
    "We'll set up some of our variables for our notebook here. \n",
    "\n",
    "If by chance you're using a private dataset, you'll also want to make sure that you have the **Google Cloud Software Development Kit (SDK)** attached to your notebook. You can find the Google Cloud SDK under the `Add-ons` dropdown menu at the top of your notebook. Documentation for the **Google Cloud Software Development Kit (SDK)** can be found **[here](https://www.kaggle.com/product-feedback/163416)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:04.546237Z",
     "iopub.status.busy": "2021-03-21T22:15:04.544421Z",
     "iopub.status.idle": "2021-03-21T22:15:04.547798Z",
     "shell.execute_reply": "2021-03-21T22:15:04.548335Z"
    },
    "papermill": {
     "duration": 0.04998,
     "end_time": "2021-03-21T22:15:04.548530",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.498550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#GCS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-classification')\n",
    "GCS_PATH = '../input/cassava-leaf-disease-classification'\n",
    "#BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = [512, 512]\n",
    "CLASSES = ['0', '1', '2', '3', '4']\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038053,
     "end_time": "2021-03-21T22:15:04.624184",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.586131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the data\n",
    "If you've primarily worked with notebooks in Learn, you've maybe noticed that data import and formatting is taken care of for you. But because we're working with competition data we'll have to handle this part of the pipeline ourselves.   \n",
    "\n",
    "The data we're working with have been formatted into `TFRecords`, which are a format for storing a sequence of binary records. `TFRecords` work _really_ well with TPUs, and allow us to send a small number of large files across the TPU for processing.   \n",
    "\n",
    "If you'd like to learn more about `TFRecords` and maybe even try creating them yourself, check out this **[TFRecords Basics notebook](https://www.kaggle.com/ryanholbrook/tfrecords-basics)** and **[corresponding video](https://youtu.be/KgjaC9VeOi8)** from Kaggle Data Scientist Ryan Holbrook.  \n",
    "\n",
    "Because our data consists of `training` and `test` images only, we're going to split our `training` data into `training` and `validation` data using the `train_test_split()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038474,
     "end_time": "2021-03-21T22:15:04.700227",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.661753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Decode the data\n",
    "In the code chunk below we'll set up a series of functions that allow us to convert our images into tensors so that we can utilize them in our model. We'll also normalize our data. Our images are using a \"Red, Blue, Green (RBG)\" scale that has a range of [0, 255], and by normalizing it we'll set each pixel's value to a number in the range of [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:04.779264Z",
     "iopub.status.busy": "2021-03-21T22:15:04.778195Z",
     "iopub.status.idle": "2021-03-21T22:15:04.783755Z",
     "shell.execute_reply": "2021-03-21T22:15:04.783123Z"
    },
    "papermill": {
     "duration": 0.047124,
     "end_time": "2021-03-21T22:15:04.783906",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.736782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038221,
     "end_time": "2021-03-21T22:15:04.860098",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.821877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you think back to **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** you might remember how we set up variables like `X` and `y`, representing our `features`, `X`, and `prediction target`, `y`. This code is accomplishing something similar, although instead of using the labels `X` and `y`, our `features` are represented by the term `image` and our `prediction target` by the term `target`.  \n",
    "\n",
    "You might also notice that this function accounts for unlabeled images. This is because our test image doesn't have any labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:04.945358Z",
     "iopub.status.busy": "2021-03-21T22:15:04.944564Z",
     "iopub.status.idle": "2021-03-21T22:15:04.949415Z",
     "shell.execute_reply": "2021-03-21T22:15:04.948800Z"
    },
    "papermill": {
     "duration": 0.049647,
     "end_time": "2021-03-21T22:15:04.949562",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.899915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    } if labeled else {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    if labeled:\n",
    "        label = tf.cast(example['target'], tf.int32)\n",
    "        return image, label\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039539,
     "end_time": "2021-03-21T22:15:05.026844",
     "exception": false,
     "start_time": "2021-03-21T22:15:04.987305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll use the following function to load our dataset. One of the advantages of a TPU is that we can run multiple files across the TPU at once, and this accounts for the speed advantages of using a TPU. To capitalize on that, we want to make sure that we're using data as soon as it streams in, rather than creating a data streaming bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.109281Z",
     "iopub.status.busy": "2021-03-21T22:15:05.108272Z",
     "iopub.status.idle": "2021-03-21T22:15:05.113727Z",
     "shell.execute_reply": "2021-03-21T22:15:05.113191Z"
    },
    "papermill": {
     "duration": 0.049451,
     "end_time": "2021-03-21T22:15:05.113857",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.064406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036959,
     "end_time": "2021-03-21T22:15:05.188575",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.151616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## A note on using train_test_split()\n",
    "While I used `train_test_split()` to create both a `training` and `validation` dataset, consider exploring **[cross validation instead](https://www.kaggle.com/dansbecker/cross-validation)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.267836Z",
     "iopub.status.busy": "2021-03-21T22:15:05.266611Z",
     "iopub.status.idle": "2021-03-21T22:15:05.290170Z",
     "shell.execute_reply": "2021-03-21T22:15:05.289196Z"
    },
    "papermill": {
     "duration": 0.064979,
     "end_time": "2021-03-21T22:15:05.290304",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.225325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n",
    "    tf.io.gfile.glob(GCS_PATH + '/train_tfrecords/ld_train*.tfrec'),\n",
    "    test_size=0.9, random_state=42\n",
    ")\n",
    "\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test_tfrecords/ld_test*.tfrec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038606,
     "end_time": "2021-03-21T22:15:05.368269",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.329663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Adding in augmentations \n",
    "You learned about augmentations in the **[Computer Vision: Data Augmentation](https://www.kaggle.com/ryanholbrook/data-augmentation)** lesson on Kaggle Learn, and here I've applied an augmentation available to us through TensorFlow. You can read more about these augmentations (as well as all of the other augmentations available to you!) in the **[TensorFlow tf.image documentation](https://www.tensorflow.org/api_docs/python/tf/image)**.  \n",
    "\n",
    "If you're interested in learning how to create and use custom augmentations, check out these **[Rotation Augmentation GPU/TPU](https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96)** and **[CutMix and MixUp on GPU/TPU](https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu)** from Kaggle Grandmaster Chris Deotte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.457175Z",
     "iopub.status.busy": "2021-03-21T22:15:05.454469Z",
     "iopub.status.idle": "2021-03-21T22:15:05.457828Z",
     "shell.execute_reply": "2021-03-21T22:15:05.458364Z"
    },
    "papermill": {
     "duration": 0.051345,
     "end_time": "2021-03-21T22:15:05.458501",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.407156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def center_crop(image, label):\n",
    "    image = tf.image.central_crop(image, central_fraction=400/512)\n",
    "    return image, label\n",
    "\n",
    "def data_augment(image, label):\n",
    "    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n",
    "    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n",
    "    seed = (1, 2)\n",
    "    #image = tf.image.central_crop(image, central_fraction=0.6)\n",
    "    image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "    image = tf.image.stateless_random_flip_up_down(image, seed)\n",
    "    image = tf.image.stateless_random_brightness(image, 0.2, seed)\n",
    "    image = tf.image.stateless_random_contrast(image, 0.2, 0.7, seed)\n",
    "    image = tf.image.stateless_random_crop(image, size=(400, 400, 3), seed=seed)\n",
    "    image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "    image = tf.image.stateless_random_saturation(image, 0.2, 0.7, seed)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "IMAGESIZE2 = [400,400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03836,
     "end_time": "2021-03-21T22:15:05.534158",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.495798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define data loading methods\n",
    "The following functions will be used to load our `training`, `validation`, and `test` datasets, as well as print out the number of images in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.618022Z",
     "iopub.status.busy": "2021-03-21T22:15:05.615954Z",
     "iopub.status.idle": "2021-03-21T22:15:05.618709Z",
     "shell.execute_reply": "2021-03-21T22:15:05.619355Z"
    },
    "papermill": {
     "duration": 0.048318,
     "end_time": "2021-03-21T22:15:05.619539",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.571221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)  \n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  # apply image aug\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.701949Z",
     "iopub.status.busy": "2021-03-21T22:15:05.701284Z",
     "iopub.status.idle": "2021-03-21T22:15:05.706536Z",
     "shell.execute_reply": "2021-03-21T22:15:05.705980Z"
    },
    "papermill": {
     "duration": 0.048834,
     "end_time": "2021-03-21T22:15:05.706656",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.657822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) \n",
    "    dataset = dataset.map(center_crop, num_parallel_calls=AUTOTUNE)  # center crop\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.787835Z",
     "iopub.status.busy": "2021-03-21T22:15:05.787221Z",
     "iopub.status.idle": "2021-03-21T22:15:05.791769Z",
     "shell.execute_reply": "2021-03-21T22:15:05.792359Z"
    },
    "papermill": {
     "duration": 0.049804,
     "end_time": "2021-03-21T22:15:05.792548",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.742744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.map(center_crop, num_parallel_calls=AUTOTUNE)  # center crop\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.876960Z",
     "iopub.status.busy": "2021-03-21T22:15:05.874123Z",
     "iopub.status.idle": "2021-03-21T22:15:05.877713Z",
     "shell.execute_reply": "2021-03-21T22:15:05.878195Z"
    },
    "papermill": {
     "duration": 0.047256,
     "end_time": "2021-03-21T22:15:05.878406",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.831150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:05.962766Z",
     "iopub.status.busy": "2021-03-21T22:15:05.960445Z",
     "iopub.status.idle": "2021-03-21T22:15:05.963525Z",
     "shell.execute_reply": "2021-03-21T22:15:05.964049Z"
    },
    "papermill": {
     "duration": 0.045746,
     "end_time": "2021-03-21T22:15:05.964236",
     "exception": false,
     "start_time": "2021-03-21T22:15:05.918490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "# NUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\n",
    "# NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "\n",
    "# print('Dataset: {} training images, {} validation images, {} (unlabeled) test images'.format(\n",
    "#     NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037117,
     "end_time": "2021-03-21T22:15:06.040685",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.003568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Building the model\n",
    "## Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.121961Z",
     "iopub.status.busy": "2021-03-21T22:15:06.121152Z",
     "iopub.status.idle": "2021-03-21T22:15:06.126130Z",
     "shell.execute_reply": "2021-03-21T22:15:06.125612Z"
    },
    "papermill": {
     "duration": 0.048352,
     "end_time": "2021-03-21T22:15:06.126251",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.077899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initial_learning_rate = 1e-4\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#       initial_learning_rate,\n",
    "#       decay_steps=1000,\n",
    "#       decay_rate=0.96)\n",
    "# # Callback\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=15, restore_best_weights=True)\n",
    "# #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "\n",
    "\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# if not os.path.isdir(save_dir):\n",
    "#       os.makedirs(save_dir)\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=save_dir,\n",
    "#     monitor='val_sparse_categorical_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "\n",
    "# # Model weights are saved at the end of every epoch, if it's the best seen\n",
    "# # so far.\n",
    "# model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037576,
     "end_time": "2021-03-21T22:15:06.200908",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.163332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building our model\n",
    "In order to ensure that our model is trained on the TPU, we build it using `with strategy.scope()`.    \n",
    "\n",
    "This model was built using transfer learning, meaning that we have a _pre-trained model_ (ResNet50) as our base model and then the customizable model built using `tf.keras.Sequential`. If you're new to transfer learning I recommend setting `base_model.trainable` to **False**, but _do_ encourage you to change which base model you're using (more options are available in the **[`tf.keras.applications` Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications)** documentation) as well iterate on the custom model. \n",
    "\n",
    "Note that we're using `sparse_categorical_crossentropy` as our loss function, because we did _not_ one-hot encode our labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037385,
     "end_time": "2021-03-21T22:15:06.277749",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.240364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.362326Z",
     "iopub.status.busy": "2021-03-21T22:15:06.361759Z",
     "iopub.status.idle": "2021-03-21T22:15:06.366035Z",
     "shell.execute_reply": "2021-03-21T22:15:06.365569Z"
    },
    "papermill": {
     "duration": 0.048222,
     "end_time": "2021-03-21T22:15:06.366155",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.317933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.448183Z",
     "iopub.status.busy": "2021-03-21T22:15:06.445639Z",
     "iopub.status.idle": "2021-03-21T22:15:06.449071Z",
     "shell.execute_reply": "2021-03-21T22:15:06.449612Z"
    },
    "papermill": {
     "duration": 0.046544,
     "end_time": "2021-03-21T22:15:06.449769",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.403225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def f1_m(y_true, y_pred):\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     recall = true_positives / (possible_positives + K.epsilon())\n",
    "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.525144Z",
     "iopub.status.busy": "2021-03-21T22:15:06.524564Z",
     "iopub.status.idle": "2021-03-21T22:15:06.529573Z",
     "shell.execute_reply": "2021-03-21T22:15:06.529020Z"
    },
    "papermill": {
     "duration": 0.044639,
     "end_time": "2021-03-21T22:15:06.529703",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.485064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def build_res(lr,alpha,drop_rate):\n",
    "   \n",
    "#     img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet50.preprocess_input, input_shape=[*IMAGESIZE2, 3])\n",
    "\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "#     base_model.trainable = False\n",
    "# #     for x in base_model.layers[1:30]:  # Unfreeze some layers: no i cant\n",
    "# #         x.trainable = True\n",
    "\n",
    "#     model_res = tf.keras.Sequential([\n",
    "#         #tf.keras.layers.BatchNormalization(renorm=True),\n",
    "#         img_adjust_layer,\n",
    "#         base_model,\n",
    "#         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(512, activation = \"relu\", kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(5, activation = 'softmax')\n",
    "#     ])\n",
    "#     model_res.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=lr, epsilon=0.001),\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "#     return model_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.608937Z",
     "iopub.status.busy": "2021-03-21T22:15:06.606608Z",
     "iopub.status.idle": "2021-03-21T22:15:06.609834Z",
     "shell.execute_reply": "2021-03-21T22:15:06.610284Z"
    },
    "papermill": {
     "duration": 0.043868,
     "end_time": "2021-03-21T22:15:06.610430",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.566562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# with strategy.scope():  \n",
    "#     model_res = load_model(\"../input/resmodel/resmodel_2.h5\")\n",
    "#     model_res.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=0.001),\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.687242Z",
     "iopub.status.busy": "2021-03-21T22:15:06.686517Z",
     "iopub.status.idle": "2021-03-21T22:15:06.690860Z",
     "shell.execute_reply": "2021-03-21T22:15:06.690338Z"
    },
    "papermill": {
     "duration": 0.043408,
     "end_time": "2021-03-21T22:15:06.690993",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.647585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lr = 1e-4\n",
    "# alpha = 0.015\n",
    "# drop_rate = 0.2\n",
    "\n",
    "\n",
    "\n",
    "# with strategy.scope():   \n",
    "#     model_res = build_res(lr_schedule, alpha, drop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.771088Z",
     "iopub.status.busy": "2021-03-21T22:15:06.768746Z",
     "iopub.status.idle": "2021-03-21T22:15:06.771862Z",
     "shell.execute_reply": "2021-03-21T22:15:06.772388Z"
    },
    "papermill": {
     "duration": 0.045034,
     "end_time": "2021-03-21T22:15:06.772601",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.727567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = get_training_dataset()\n",
    "# valid_dataset = get_validation_dataset()\n",
    "\n",
    "# STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "# VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n",
    "# with strategy.scope(): \n",
    "#     h_res = model_res.fit(train_dataset, \n",
    "#                           steps_per_epoch=STEPS_PER_EPOCH, \n",
    "#                           epochs=EPOCHS,\n",
    "#                           validation_data=valid_dataset,\n",
    "#                           validation_steps=VALID_STEPS,\n",
    "#                           callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035498,
     "end_time": "2021-03-21T22:15:06.844281",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.808783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LR schedule callback to determine lr: 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:06.924539Z",
     "iopub.status.busy": "2021-03-21T22:15:06.923631Z",
     "iopub.status.idle": "2021-03-21T22:15:06.927252Z",
     "shell.execute_reply": "2021-03-21T22:15:06.926759Z"
    },
    "papermill": {
     "duration": 0.046347,
     "end_time": "2021-03-21T22:15:06.927372",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.881025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# def build_res(lr,alpha,drop_rate):\n",
    "   \n",
    "#     img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet50.preprocess_input, input_shape=[*IMAGESIZE2, 3])\n",
    "\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "#     base_model.trainable = False\n",
    "# #     for x in base_model.layers[-31:-1]:  # Unfreeze some layers\n",
    "# #         x.trainable = True\n",
    "\n",
    "#     model_res = tf.keras.Sequential([\n",
    "#         tf.keras.layers.BatchNormalization(renorm=True),\n",
    "#         img_adjust_layer,\n",
    "#         base_model,\n",
    "#         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(512, activation = \"relu\", kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(5, activation = 'softmax')\n",
    "#     ])\n",
    "#     model_res.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=lr, epsilon=0.001),\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "#     return model_res\n",
    "\n",
    "\n",
    "\n",
    "# with strategy.scope():   \n",
    "#     model_res = build_res(1e-8, 0.015, 0.2)\n",
    "\n",
    "#     # Callback\n",
    "#     #callback = tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=20, restore_best_weights=True)\n",
    "#     lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8*10**(epoch/20))\n",
    "#     history = model_res.fit(train_dataset, \n",
    "#                           steps_per_epoch=STEPS_PER_EPOCH, \n",
    "#                           epochs=100,\n",
    "#                           validation_data=valid_dataset,\n",
    "#                           validation_steps=VALID_STEPS,\n",
    "#                           callbacks=[lr_schedule])\n",
    "#     lrs = 1e-8*10**(np.arange(100)/20)\n",
    "#     plt.semilogx(lrs, history.history[\"loss\"], label='train')\n",
    "#     plt.semilogx(lrs, history.history[\"val_loss\"], label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:07.007596Z",
     "iopub.status.busy": "2021-03-21T22:15:07.006597Z",
     "iopub.status.idle": "2021-03-21T22:15:07.010302Z",
     "shell.execute_reply": "2021-03-21T22:15:07.009814Z"
    },
    "papermill": {
     "duration": 0.045655,
     "end_time": "2021-03-21T22:15:07.010430",
     "exception": false,
     "start_time": "2021-03-21T22:15:06.964775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# def build_res(alpha,drop_rate):\n",
    "   \n",
    "#     img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet50.preprocess_input, input_shape=[*IMAGESIZE2, 3])\n",
    "\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "#     base_model.trainable = False\n",
    "# #     for x in base_model.layers[-31:-1]:  # Unfreeze some layers\n",
    "# #         x.trainable = True\n",
    "\n",
    "#     model_res = tf.keras.Sequential([\n",
    "#         tf.keras.layers.BatchNormalization(renorm=True),\n",
    "#         img_adjust_layer,\n",
    "#         base_model,\n",
    "#         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(512, activation = \"relu\", kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(5, activation = 'softmax')\n",
    "#     ])\n",
    "    \n",
    "#     return model_res\n",
    "\n",
    "# def fit_model(model, train_dataset, valid_dataset, decay_step):\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#           initial_learning_rate = 1e-4,\n",
    "#           decay_steps= decay_step,\n",
    "#           decay_rate=0.96)\n",
    "#     # compile model\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, epsilon=0.001),\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['sparse_categorical_accuracy'])\n",
    "#     # fit model\n",
    "#     STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "#     VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n",
    "#     h_res = model_res.fit(train_dataset, \n",
    "#                       steps_per_epoch=STEPS_PER_EPOCH, \n",
    "#                       epochs=15,\n",
    "#                       validation_data=valid_dataset,\n",
    "#                       validation_steps=VALID_STEPS)\n",
    "#     # plot learning curves\n",
    "#     plt.plot(h_res.history['sparse_categorical_accuracy'], label='train')\n",
    "#     plt.plot(h_res.history['val_sparse_categorical_accuracy'], label='validation')\n",
    "#     plt.title('initial lrate='+str(decay_step))\n",
    "\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# with strategy.scope():   \n",
    "#     # create learning curves for different learning rates\n",
    "#     decay_steps = [1, 10, 1e2, 1e3, 1e4, 1e5]\n",
    "#     for i in range(len(decay_steps)):\n",
    "#         model = build_res(alpha,drop_rate)\n",
    "#         # determine the plot number\n",
    "#         plot_no = 420 + (i+1)\n",
    "#         plt.subplot(plot_no)\n",
    "#         # fit model and plot learning curves for a learning rate\n",
    "#         fit_model(model, train_dataset, valid_dataset, decay_steps[i])\n",
    "#     # show learning curves\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:07.091265Z",
     "iopub.status.busy": "2021-03-21T22:15:07.090259Z",
     "iopub.status.idle": "2021-03-21T22:15:07.094063Z",
     "shell.execute_reply": "2021-03-21T22:15:07.093545Z"
    },
    "papermill": {
     "duration": 0.046463,
     "end_time": "2021-03-21T22:15:07.094188",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.047725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# def build_res(alpha,drop_rate):\n",
    "   \n",
    "#     img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet50.preprocess_input, input_shape=[*IMAGESIZE2, 3])\n",
    "\n",
    "#     base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "#     base_model.trainable = False\n",
    "# #     for x in base_model.layers[-31:-1]:  # Unfreeze some layers\n",
    "# #         x.trainable = True\n",
    "\n",
    "#     model_res = tf.keras.Sequential([\n",
    "#         tf.keras.layers.BatchNormalization(renorm=True),\n",
    "#         img_adjust_layer,\n",
    "#         base_model,\n",
    "#         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(512, activation = \"relu\", kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(alpha)),\n",
    "#         tf.keras.layers.BatchNormalization(momentum=0.97),\n",
    "#         tf.keras.layers.Dropout(rate=drop_rate),\n",
    "#         tf.keras.layers.Dense(5, activation = 'softmax')\n",
    "#     ])\n",
    "    \n",
    "#     return model_res\n",
    "\n",
    "# def fit_model(model, train_dataset, valid_dataset, opt, callback):\n",
    "    \n",
    "#     # compile model\n",
    "#     model.compile(\n",
    "#         optimizer=opt,\n",
    "#         loss='sparse_categorical_crossentropy',\n",
    "#         metrics=['sparse_categorical_accuracy'])\n",
    "#     # fit model\n",
    "#     STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "#     VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE\n",
    "#     h_res = model_res.fit(train_dataset, \n",
    "#                       steps_per_epoch=STEPS_PER_EPOCH, \n",
    "#                       epochs=15,\n",
    "#                       validation_data=valid_dataset,\n",
    "#                       validation_steps=VALID_STEPS,\n",
    "#                       callbacks=[callback])\n",
    "#     # plot learning curves\n",
    "#     plt.plot(h_res.history['sparse_categorical_accuracy'], label='train')\n",
    "#     plt.plot(h_res.history['val_sparse_categorical_accuracy'], label='validation')\n",
    "#     plt.title('Optimizer='+str(opt))\n",
    "\n",
    "# initial_learning_rate = 1e-4\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#           initial_learning_rate,\n",
    "#           decay_steps=1000,\n",
    "#           decay_rate=0.96,\n",
    "#           staircase=True) \n",
    "# opt_adagrad = tf.keras.optimizers.Adagrad(learning_rate=lr_schedule, epsilon=0.001)\n",
    "# opt_adam = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=0.001)\n",
    "# opt_adadelta = tf.keras.optimizers.Adadelta(learning_rate=lr_schedule, epsilon=0.001)\n",
    "# opt_rmsprop = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule, epsilon=0.001)\n",
    "\n",
    "# with strategy.scope():   \n",
    "#     # create learning curves for different learning rates\n",
    "#     opts = [opt_adagrad,opt_adam, opt_adadelta,opt_rmsprop]\n",
    "#     for i in range(len(opts)):\n",
    "#         model = build_res(alpha,drop_rate)\n",
    "#         # determine the plot number\n",
    "#         plot_no = 420 + (i+1)\n",
    "#         plt.subplot(plot_no)\n",
    "#         # fit model and plot learning curves for a learning rate\n",
    "#         fit_model(model, train_dataset, valid_dataset, opts[i], callback)\n",
    "#     # show learning curves\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038187,
     "end_time": "2021-03-21T22:15:07.168982",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.130795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With model.summary() we'll see a printout of each of our layers, their corresponding shape, as well as the associated number of parameters. Notice that at the bottom of the printout we'll see information on the total parameters, trainable parameters, and non-trainable parameters. Because we're using a pre-trained model, we expect there to be a large number of non-trainable parameters (because the weights have already been assigned in the pre-trained model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037072,
     "end_time": "2021-03-21T22:15:07.243997",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.206925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluating our model\n",
    "The first chunk of code is provided to show you where the variables in the second chunk of code came from. As you can see, there's a lot of room for improvement in this model, but because we're using TPUs and have a relatively short training time, we're able to iterate on our model fairly rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:07.323557Z",
     "iopub.status.busy": "2021-03-21T22:15:07.322720Z",
     "iopub.status.idle": "2021-03-21T22:15:07.326024Z",
     "shell.execute_reply": "2021-03-21T22:15:07.326619Z"
    },
    "papermill": {
     "duration": 0.045863,
     "end_time": "2021-03-21T22:15:07.326798",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.280935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # print out variables available to us\n",
    "# print(h_res.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:07.407409Z",
     "iopub.status.busy": "2021-03-21T22:15:07.406635Z",
     "iopub.status.idle": "2021-03-21T22:15:07.410110Z",
     "shell.execute_reply": "2021-03-21T22:15:07.410706Z"
    },
    "papermill": {
     "duration": 0.045689,
     "end_time": "2021-03-21T22:15:07.410875",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.365186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # create learning curves to evaluate model performance\n",
    "# history_frame = pd.DataFrame(h_res.history)\n",
    "# history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "# history_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037788,
     "end_time": "2021-03-21T22:15:07.484483",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.446695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:10.513617Z",
     "iopub.status.busy": "2021-03-21T22:15:08.527571Z",
     "iopub.status.idle": "2021-03-21T22:15:10.797967Z",
     "shell.execute_reply": "2021-03-21T22:15:10.797437Z"
    },
    "papermill": {
     "duration": 3.27729,
     "end_time": "2021-03-21T22:15:10.798146",
     "exception": false,
     "start_time": "2021-03-21T22:15:07.520856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_set(ordered=False):\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered) \n",
    "    dataset = dataset.map(center_crop, num_parallel_calls=AUTOTUNE)  # center crop\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "trainset = train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:10.877613Z",
     "iopub.status.busy": "2021-03-21T22:15:10.876979Z",
     "iopub.status.idle": "2021-03-21T22:15:19.633461Z",
     "shell.execute_reply": "2021-03-21T22:15:19.632953Z"
    },
    "papermill": {
     "duration": 8.797751,
     "end_time": "2021-03-21T22:15:19.633623",
     "exception": false,
     "start_time": "2021-03-21T22:15:10.835872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset()\n",
    "# valid_dataset = get_validation_dataset()\n",
    "\n",
    "train = list(trainset)[0]\n",
    "X_train, y_train = train\n",
    "# valid = list(valid_dataset)[0]\n",
    "# X_test, y_test = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:19.724348Z",
     "iopub.status.busy": "2021-03-21T22:15:19.723385Z",
     "iopub.status.idle": "2021-03-21T22:15:30.160862Z",
     "shell.execute_reply": "2021-03-21T22:15:30.160310Z"
    },
    "papermill": {
     "duration": 10.487528,
     "end_time": "2021-03-21T22:15:30.161062",
     "exception": false,
     "start_time": "2021-03-21T22:15:19.673534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('res1',\n",
       "                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fea9cc08610>),\n",
       "                             ('res2',\n",
       "                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fea9c0d05d0>),\n",
       "                             ('res3',\n",
       "                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fea9c0d0bd0>)],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def get_model(mod):\n",
    "    if mod == 0:\n",
    "        model = load_model(\"../input/resmodel/resmodel.h5\")\n",
    "    elif mod == 1:\n",
    "        model = load_model(\"../input/resmodel/resmodel_2.h5\")\n",
    "    elif mod == 2:\n",
    "        model = load_model(\"../input/resmodel/resmodel_3.h5\")\n",
    "    return model\n",
    "\n",
    "# def get_model():\n",
    "#     model = load_model(\"../input/resmodel/resmodel_3.h5\")\n",
    "#     return model\n",
    "\n",
    "res1_clf = tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "                            lambda: get_model(0),\n",
    "                            epochs=0,\n",
    "                            verbose=False)\n",
    "res2_clf = tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "                            lambda: get_model(1),\n",
    "                            epochs=0,\n",
    "                            verbose=False)\n",
    "res3_clf = tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "                            lambda: get_model(2),\n",
    "                            epochs=0,\n",
    "                            verbose=False)\n",
    "\n",
    "for x in [res1_clf, res2_clf, res3_clf]:\n",
    "    x._estimator_type = \"classifier\"\n",
    "\n",
    "voting = VotingClassifier(\n",
    "             estimators=[('res1', res1_clf),\n",
    "                         ('res2', res2_clf),\n",
    "                         ('res3', res3_clf)], \n",
    "             voting='soft',\n",
    "             flatten_transform=True)\n",
    "\n",
    "\n",
    "# for clf in (res1_clf, res2_clf, res3_clf, voting):\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "#     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:30.240480Z",
     "iopub.status.busy": "2021-03-21T22:15:30.239845Z",
     "iopub.status.idle": "2021-03-21T22:15:30.244867Z",
     "shell.execute_reply": "2021-03-21T22:15:30.245349Z"
    },
    "papermill": {
     "duration": 0.046921,
     "end_time": "2021-03-21T22:15:30.245505",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.198584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save the model to disk\n",
    "# filename = 'votemodel.pkl'\n",
    "# with open(filename, 'wb') as file:  \n",
    "#     pickle.dump(voting, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038176,
     "end_time": "2021-03-21T22:15:30.323797",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.285621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making predictions\n",
    "Now that we've trained our model we can use it to make predictions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:30.405033Z",
     "iopub.status.busy": "2021-03-21T22:15:30.404236Z",
     "iopub.status.idle": "2021-03-21T22:15:30.408606Z",
     "shell.execute_reply": "2021-03-21T22:15:30.409564Z"
    },
    "papermill": {
     "duration": 0.047622,
     "end_time": "2021-03-21T22:15:30.409768",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.362146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # this code will convert our test image data to a float32 \n",
    "# def to_float32(image, label):\n",
    "#     return tf.cast(image, tf.float32), label\n",
    "\n",
    "# test_ds = get_test_dataset(ordered=True) \n",
    "# test_ds = test_ds.map(to_float32)\n",
    "\n",
    "# print('Computing predictions...')\n",
    "# #test_images_ds = test_dataset\n",
    "# test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "# probabilities = model_res.predict(test_images_ds)\n",
    "# predictions = np.argmax(probabilities, axis=-1)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037724,
     "end_time": "2021-03-21T22:15:30.484909",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.447185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:30.569997Z",
     "iopub.status.busy": "2021-03-21T22:15:30.569270Z",
     "iopub.status.idle": "2021-03-21T22:15:30.573665Z",
     "shell.execute_reply": "2021-03-21T22:15:30.574217Z"
    },
    "papermill": {
     "duration": 0.050625,
     "end_time": "2021-03-21T22:15:30.574371",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.523746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # save model\n",
    "\n",
    "# def save_model(model, name):\n",
    "#   model_name = '{}.h5'.format(name)\n",
    "#   save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "  \n",
    "#   # Save model and weights\n",
    "#   if not os.path.isdir(save_dir):\n",
    "#       os.makedirs(save_dir)\n",
    "#   model_path = os.path.join(save_dir, model_name)\n",
    "#   model.save(model_path)\n",
    "#   print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# save_model(model_res, 'resmodel_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:30.657075Z",
     "iopub.status.busy": "2021-03-21T22:15:30.656387Z",
     "iopub.status.idle": "2021-03-21T22:15:30.661384Z",
     "shell.execute_reply": "2021-03-21T22:15:30.660691Z"
    },
    "papermill": {
     "duration": 0.048197,
     "end_time": "2021-03-21T22:15:30.661530",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.613333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from keras.models import load_model\n",
    "\n",
    "# #model = load_model(\"../input/resmodel1/resmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:30.743339Z",
     "iopub.status.busy": "2021-03-21T22:15:30.742751Z",
     "iopub.status.idle": "2021-03-21T22:15:30.747581Z",
     "shell.execute_reply": "2021-03-21T22:15:30.746943Z"
    },
    "papermill": {
     "duration": 0.046849,
     "end_time": "2021-03-21T22:15:30.747708",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.700859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = pickle.load(open(filename, 'rb'))\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037836,
     "end_time": "2021-03-21T22:15:30.824699",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.786863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating a submission file\n",
    "Now that we've trained a model and made predictions we're ready to submit to the competition! You can run the following code below to get your submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:30.909929Z",
     "iopub.status.busy": "2021-03-21T22:15:30.909136Z",
     "iopub.status.idle": "2021-03-21T22:15:37.867069Z",
     "shell.execute_reply": "2021-03-21T22:15:37.865857Z"
    },
    "papermill": {
     "duration": 7.00449,
     "end_time": "2021-03-21T22:15:37.867224",
     "exception": false,
     "start_time": "2021-03-21T22:15:30.862734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "sample_sub = pd.read_csv('/kaggle/input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "\n",
    "\n",
    "for image in sample_sub.image_id:\n",
    "    img = keras.preprocessing.image.load_img('/kaggle/input/cassava-leaf-disease-classification/test_images/' + image)\n",
    "    #\n",
    "    # Preprocess image here (rescale, etc. - you might need to use parameters you determined during training)\n",
    "    \n",
    "    #\n",
    "    img = np.array(img)\n",
    "    image = tf.cast(img, tf.float32) / 255.0\n",
    "    img = tf.image.resize(image, IMAGE_SIZE, antialias=True)\n",
    "    img = tf.image.central_crop(img, central_fraction=400/512)\n",
    "    img = tf.reshape(img, (-1, 400, 400, 3))\n",
    "    # Now apply your model and save your prediction:\n",
    "    \n",
    "    prediction = voting.predict(img)[0]\n",
    "    #preds.append(np.argmax(prediction))\n",
    "    preds.append(prediction)\n",
    "    # Blind-Monkey Model\n",
    "    # This is horrible possible baseline model.  You can improve it by\n",
    "    # putting all of p's mass on the most commonly occuring class.\n",
    "    # Question: if you set p to the actual class label distribution, on average,  \n",
    "    # will you get the same result, a better result or a worse result? \n",
    "\n",
    "my_submission = pd.DataFrame({'image_id': sample_sub.image_id, 'label': preds})\n",
    "my_submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:37.951460Z",
     "iopub.status.busy": "2021-03-21T22:15:37.949085Z",
     "iopub.status.idle": "2021-03-21T22:15:37.952194Z",
     "shell.execute_reply": "2021-03-21T22:15:37.952795Z"
    },
    "papermill": {
     "duration": 0.045724,
     "end_time": "2021-03-21T22:15:37.952928",
     "exception": false,
     "start_time": "2021-03-21T22:15:37.907204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.read_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:38.031273Z",
     "iopub.status.busy": "2021-03-21T22:15:38.030542Z",
     "iopub.status.idle": "2021-03-21T22:15:38.034833Z",
     "shell.execute_reply": "2021-03-21T22:15:38.034346Z"
    },
    "papermill": {
     "duration": 0.044394,
     "end_time": "2021-03-21T22:15:38.034943",
     "exception": false,
     "start_time": "2021-03-21T22:15:37.990549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #from skimage.transform import resize\n",
    "\n",
    "# img = keras.preprocessing.image.load_img('/kaggle/input/cassava-leaf-disease-classification/test_images/' + sample_sub.image_id[0])\n",
    "# img = np.array(img)\n",
    "# # img = resize(image, (512, 512),anti_aliasing=True)\n",
    "# # img = tf.reshape(img, (-1, 512, 512, 3))\n",
    "# image = tf.cast(img, tf.float32) / 255.0\n",
    "# img = tf.image.resize(image, IMAGE_SIZE, antialias=True)\n",
    "# img = tf.image.central_crop(img, central_fraction=400/512)\n",
    "# img = tf.reshape(img, (-1, 400, 400, 3))\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:38.117364Z",
     "iopub.status.busy": "2021-03-21T22:15:38.116787Z",
     "iopub.status.idle": "2021-03-21T22:15:38.121454Z",
     "shell.execute_reply": "2021-03-21T22:15:38.120809Z"
    },
    "papermill": {
     "duration": 0.047473,
     "end_time": "2021-03-21T22:15:38.121640",
     "exception": false,
     "start_time": "2021-03-21T22:15:38.074167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# voting.predict(img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:38.204348Z",
     "iopub.status.busy": "2021-03-21T22:15:38.203585Z",
     "iopub.status.idle": "2021-03-21T22:15:38.209402Z",
     "shell.execute_reply": "2021-03-21T22:15:38.208778Z"
    },
    "papermill": {
     "duration": 0.048723,
     "end_time": "2021-03-21T22:15:38.209523",
     "exception": false,
     "start_time": "2021-03-21T22:15:38.160800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('Generating submission.csv file...')\n",
    "# test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "# test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "# np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n",
    "# !head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039219,
     "end_time": "2021-03-21T22:15:38.287963",
     "exception": false,
     "start_time": "2021-03-21T22:15:38.248744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Be aware that because this is a code competition with a hidden test set, internet and TPUs cannot be enabled on your submission notebook. Therefore TPUs will only be available for training models. For a walk-through on how to train on TPUs and run inference/submit on GPUs, see our [TPU Docs](https://www.kaggle.com/docs/tpu#tpu6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038429,
     "end_time": "2021-03-21T22:15:38.364117",
     "exception": false,
     "start_time": "2021-03-21T22:15:38.325688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T22:15:38.447341Z",
     "iopub.status.busy": "2021-03-21T22:15:38.444835Z",
     "iopub.status.idle": "2021-03-21T22:15:38.448043Z",
     "shell.execute_reply": "2021-03-21T22:15:38.448611Z"
    },
    "papermill": {
     "duration": 0.044457,
     "end_time": "2021-03-21T22:15:38.448822",
     "exception": false,
     "start_time": "2021-03-21T22:15:38.404365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# test_path = '../input/cassava-leaf-disease-classification/test_images'\n",
    "\n",
    "# test_images = os.listdir(test_path)\n",
    "# predictions = []\n",
    "\n",
    "# for image_id in test_images:\n",
    "    \n",
    "#     image = Image.open(os.path.join(test_path, image_id))\n",
    "#     image = np.array(image)\n",
    "#     image = np.expand_dims(image, axis=0)\n",
    "# #     print(image.shape)\n",
    "#     predictions.append(np.argmax(model_res.predict(image)))\n",
    "\n",
    "# sub = pd.DataFrame({'image_id': test_images, 'label': predictions})\n",
    "# sub.to_csv(os.path.join(RESULTSPATH, 'submission.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.839504,
   "end_time": "2021-03-21T22:15:41.925211",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-21T22:14:51.085707",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
